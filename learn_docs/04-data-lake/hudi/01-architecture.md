# Apache Hudi 架构详解

## 目录

- [概述](#概述)
- [核心概念](#核心概念)
- [表类型](#表类型)
- [查询类型](#查询类型)
- [架构原理](#架构原理)

---

## 概述

Apache Hudi 是一个开源的数据湖框架，支持增量处理和统一存储。

### 核心特性

| 特性 | 说明 |
|------|------|
| **增量处理** | 支持 Upsert 和增量查询 |
| **事务支持** | ACID 事务 |
| **时间旅行** | 查询历史版本 |
| **表格式** | 支持多种查询引擎 |
| **数据布局** | 多种文件组织方式 |

---

## 核心概念

### 核心术语

| 术语 | 说明 |
|------|------|
| **Timeline** | 时间线，记录所有操作 |
| **Instant** | 瞬时操作 (Commit/Compaction/Clean) |
| **File Group** | 文件组，同一 key 的数据 |
| **File Slice** | 文件切片，一个时刻的数据文件 |
| **Base File** | Parquet 数据文件 |
| **Log File** | 增量日志文件 |

---

## 表类型

### Copy On Write (COW)

```
┌─────────────────────────────────────────────────────────────────┐
│                    Copy On Write                                │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  特点:                                                          │
│  - 写时复制                                                     │
│  - 读取优化                                                     │
│  - 适合读多写少场景                                             │
│                                                                 │
│  适用场景:                                                      │
│  - 批处理导入                                                   │
│  - 频繁读取                                                     │
│  - 少量更新                                                     │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Merge On Read (MOR)

```
┌─────────────────────────────────────────────────────────────────┐
│                    Merge On Read                                │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  特点:                                                          │
│  - 读时合并                                                     │
│  - 写时优化                                                     │
│  - 适合写多读少场景                                             │
│                                                                 │
│  适用场景:                                                      │
│  - 流式写入                                                     │
│  - 频繁更新                                                     │
│  - 近实时分析                                                   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 查询类型

### Snapshot Query

```sql
-- 读取最新版本的完整数据
SELECT * FROM hudi_table;
```

### Incremental Query

```scala
// 增量查询 (只读取变更数据)
spark.read.format("hudi")
  .option("as.of.instant", "2024-01-10 12:00:00")
  .load("/path/to/hudi/table")

// 增量查询 (自指定时间以来的变更)
spark.read.format("hudi")
  .option("begin.instant.time", "20240110120000")
  .load("/path/to/hudi/table")
```

### Read Optimized Query

```scala
// 读优化查询 (只读取 Base File)
spark.read.format("hudi")
  .option("hoodie.query.type", "read_optimized")
  .load("/path/to/hudi/table")
```

---

## 架构原理

### 文件布局

```
┌─────────────────────────────────────────────────────────────────┐
│                      Hudi Data Lake                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │                    File Layout                             │  │
│  │  ┌─────────────────────────────────────────────────────┐  │  │
│  │  │  Partition 1                                        │  │  │
│  │  │  ├── base/                                          │  │  │
│  │  │  │   ├── par_1.parquet                              │  │  │
│  │  │  │   └── par_2.parquet                              │  │  │
│  │  │  └── log/                                           │  │  │
│  │  │      ├── log_1.log                                  │  │  │
│  │  │      └── log_2.log                                  │  │  │
│  │  └─────────────────────────────────────────────────────┘  │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                 │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │                    Timeline                                │  │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐                    │  │
│  │  │Commit   │  │Commit   │  │Commit   │                    │  │
│  │  │Instant  │  │Instant  │  │Instant  │                    │  │
│  │  │t=1      │  │t=2      │  │t=3      │                    │  │
│  │  │ACTION   │  │ACTION   │  │ACTION   │                    │  │
│  │  │COMMITS  │  │COMMITS  │  │COMMITS  │                    │  │
│  │  │  1000   │  │  2000   │  │  3000   │                    │  │
│  │  └─────────┘  └─────────┘  └─────────┘                    │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Timeline 机制

```scala
// Instant 状态
// - REQUESTED: 操作请求中
// - INFLIGHT: 操作进行中
// - COMPLETED: 操作完成

// Instant 类型
// - COMMIT: 数据提交
// - COMPACTION: 文件合并
// - CLEANUP: 清理旧数据
// - ROLLBACK: 回滚操作
```

---

## 相关文档

| 文档 | 说明 |
|------|------|
| [02-data-operations.md](02-data-operations.md) | 数据操作指南 |
| [03-flink-integration.md](03-flink-integration.md) | Flink 集成 |
| [04-spark-integration.md](04-spark-integration.md) | Spark 集成 |
| [README.md](README.md) | 索引文档 |
